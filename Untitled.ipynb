{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm_data:  (98310, 2)\n",
      "log_return_fea.shape[0] (98309, 2)\n",
      "log_return_tar: (98276,)\n",
      "check nan: False\n",
      "check inf: False\n",
      "feature_lst shape (should same with target_lst):  (98276, 29, 2)\n",
      "---start to create pyh5 file---\n",
      "---done---\n",
      "WARNING:tensorflow:From /Users/haoranyu/.local/share/virtualenvs/second_project-CEnMExsQ/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/haoranyu/.local/share/virtualenvs/second_project-CEnMExsQ/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 112, 112, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 56, 56, 32)   4736        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 56, 56, 32)   0           conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 28, 28, 32)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, 28, 28, 32)   1056        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, 28, 28, 32)   128         res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 28, 28, 32)   0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, 28, 28, 32)   9248        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (None, 28, 28, 32)   128         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 28, 28, 32)   0           bn2a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, 28, 28, 128)  4224        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)         (None, 28, 28, 128)  4224        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (None, 28, 28, 128)  512         res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizati (None, 28, 28, 128)  512         res2a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 28, 28, 128)  0           bn2a_branch1[0][0]               \n",
      "                                                                 bn2a_branch2c[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 28, 28, 128)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (None, 14, 14, 64)   8256        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizati (None, 14, 14, 64)   256         res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 14, 14, 64)   0           bn3a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (None, 14, 14, 64)   36928       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizati (None, 14, 14, 64)   256         res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 14, 14, 64)   0           bn3a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (None, 14, 14, 256)  33024       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)         (None, 14, 14, 256)  16640       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalizatio (None, 14, 14, 256)  1024        res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizati (None, 14, 14, 256)  1024        res3a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 14, 14, 256)  0           bn3a_branch1[0][0]               \n",
      "                                                                 bn3a_branch2c[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 14, 14, 256)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)         (None, 7, 7, 128)    32896       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizati (None, 7, 7, 128)    512         res4a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 7, 7, 128)    0           bn4a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)         (None, 7, 7, 128)    147584      activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizati (None, 7, 7, 128)    512         res4a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 7, 7, 128)    0           bn4a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)          (None, 7, 7, 512)    131584      activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)         (None, 7, 7, 512)    66048       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalizatio (None, 7, 7, 512)    2048        res4a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizati (None, 7, 7, 512)    2048        res4a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 7, 7, 512)    0           bn4a_branch1[0][0]               \n",
      "                                                                 bn4a_branch2c[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 7, 7, 512)    0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 29, 2)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 512)    0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 29, 500)      1006000     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 512)          0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 500)          2002000     lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 500)          256500      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 500)          250500      lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1000)         0           dense_1[0][0]                    \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 500)          500500      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 500)          0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 100)          50100       dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 100)          0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 25)           2525        dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 25)           0           dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 1)            26          dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,573,559\n",
      "Trainable params: 4,569,079\n",
      "Non-trainable params: 4,480\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "debug1\n",
      "WARNING:tensorflow:From /Users/haoranyu/.local/share/virtualenvs/second_project-CEnMExsQ/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "4299/4299 [==============================] - 1910s 444ms/step - loss: 0.0013 - val_loss: 3.2786e-07\n",
      "Epoch 2/2\n",
      " 852/4299 [====>.........................] - ETA: 25:12 - loss: 7.1775e-07"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-61c4a47a7aea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    304\u001b[0m                             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                             \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfinal_val_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m                             \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_size\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size_is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m                             \u001b[0;31m#verbose=1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                             \u001b[0;31m#callbacks=callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/second_project-CEnMExsQ/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/second_project-CEnMExsQ/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/second_project-CEnMExsQ/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/second_project-CEnMExsQ/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/second_project-CEnMExsQ/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/second_project-CEnMExsQ/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/second_project-CEnMExsQ/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from keras.layers import Concatenate,Flatten,Add,Activation,BatchNormalization,Conv2D,Dense, Dropout, LSTM,Input,ZeroPadding2D,AveragePooling2D,MaxPooling2D\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.losses import mse\n",
    "import os\n",
    "import h5py\n",
    "from mpl_finance import candlestick_ohlc\n",
    "from mpl_finance import candlestick2_ohlc\n",
    "from mpl_finance import volume_overlay2\n",
    "from mpl_finance import volume_overlay\n",
    "from mpl_finance import index_bar\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import cv2\n",
    "import pickle\n",
    "\n",
    "epochs = 16\n",
    "batch_size_is = 16\n",
    "\n",
    "def convolutional_block(X, f, filters, stage, block, s = 2):\n",
    "    \"\"\"\n",
    "    Implementation of the convolutional block as defined in Figure 4\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
    "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "    stage -- integer, used to name the layers, depending on their position in the network\n",
    "    block -- string/character, used to name the layers, depending on their position in the network\n",
    "    s -- Integer, specifying the stride to be used\n",
    "    \n",
    "    Returns:\n",
    "    X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    # defining name basis\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    \n",
    "    # Retrieve Filters\n",
    "    F1, F2, F3 = filters\n",
    "    \n",
    "    # Save the input value\n",
    "    X_shortcut = X\n",
    "\n",
    "\n",
    "    ##### MAIN PATH #####\n",
    "    # First component of main path \n",
    "    X = Conv2D(F1, (1, 1), strides = (s,s), name = conv_name_base + '2a', kernel_initializer = glorot_uniform())(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Second component of main path (≈3 lines)\n",
    "    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1,1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform())(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "\n",
    "    # Third component of main path (≈2 lines)\n",
    "    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform())(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
    "    \n",
    "    \n",
    "    ##### SHORTCUT PATH #### (≈2 lines)\n",
    "    X_shortcut = Conv2D(filters = F3, kernel_size = (1, 1), strides = (s,s), padding = 'valid', name = conv_name_base + '1', kernel_initializer = glorot_uniform())(X_shortcut)\n",
    "    \n",
    "    X_shortcut = BatchNormalization(axis = 3, name = bn_name_base + '1')(X_shortcut)\n",
    "\n",
    "    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n",
    "    X = Add()([X_shortcut,X])\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "def combine_model(multi_gpu=True,num_gpus=4):\n",
    "    img_X_input=Input(shape=(112,112,3))\n",
    "    lstm_X_input = Input(shape=(29,2))\n",
    "    img_X = Conv2D(32, (7, 7), padding='same',strides = (2, 2), name = 'conv1', kernel_initializer = glorot_uniform())(img_X_input)\n",
    "    #img_X = BatchNormalization(axis = 3, name = 'bn_conv1')(img_X)\n",
    "    img_X = Activation('relu')(img_X)\n",
    "    img_X = MaxPooling2D((3, 3),strides=(2, 2),padding='same')(img_X)\n",
    "    img_X = convolutional_block(img_X, f = 3, filters = [32, 32, 128], stage = 2, block='a', s = 1)\n",
    "    img_X = convolutional_block(img_X, f = 3, filters = [64, 64, 256], stage = 3, block='a', s = 2)\n",
    "    img_X = convolutional_block(img_X, f = 3, filters = [128, 128, 512], stage = 4, block='a', s = 2)\n",
    "    img_X=AveragePooling2D(pool_size=(7, 7), strides=None, padding='valid')(img_X)\n",
    "    img_X=Flatten()(img_X)\n",
    "    img_X=Dense(500,activation='relu')(img_X)\n",
    "    \n",
    "    #img_X_more=img_X\n",
    "    img_X_more=Dropout(0.5)(img_X)\n",
    "    img_X_more=Dense(100,activation='relu')(img_X_more)\n",
    "    img_X_more=Dropout(0.5)(img_X_more)\n",
    "    img_X_more=Dense(25,activation='relu')(img_X_more)\n",
    "    img_X_more=Dropout(0.5)(img_X_more)\n",
    "    img_X_more=Dense(1)(img_X_more)\n",
    "    \n",
    "    \n",
    "    lstm_X=LSTM(units=500, return_sequences=True)(lstm_X_input)\n",
    "    lstm_X=LSTM(units=500)(lstm_X)\n",
    "    lstm_X=Dense(500,activation='relu')(lstm_X)\n",
    "    \n",
    "    #lstm_X_more=lstm_X\n",
    "    lstm_X_more=Dropout(0.5)(lstm_X)\n",
    "    lstm_X_more=Dense(100,activation='relu')(lstm_X_more)\n",
    "    lstm_X_more=Dropout(0.5)(lstm_X_more)\n",
    "    lstm_X_more=Dense(25,activation='relu')(lstm_X_more)\n",
    "    lstm_X_more=Dropout(0.5)(lstm_X_more)\n",
    "    lstm_X_more=Dense(1)(lstm_X_more)\n",
    "    #consider output will reduce neuron again \n",
    "    fused=Concatenate()([img_X,lstm_X])\n",
    "    fused=Dense(500,activation='relu')(fused)\n",
    "    fused=Dropout(0.5)(fused)\n",
    "    fused=Dense(100,activation='relu')(fused)\n",
    "    fused=Dropout(0.5)(fused)\n",
    "    fused=Dense(25,activation='relu')(fused)\n",
    "    fused=Dropout(0.5)(fused)\n",
    "    fused=Dense(1)(fused)\n",
    "    model = Model(inputs=[img_X_input,lstm_X_input],outputs=fused)\n",
    "    \n",
    "    def root_mean_squared_error(y_true, y_pred):\n",
    "        #print (\"y_ture: \", y_true)\n",
    "        #print (\"y_pred: \", y_pred)\n",
    "        #print(\"img_X\",img_X.shape)\n",
    "        #img_X,lstm_X,fused=combine_model()\n",
    "        #loss1=K.sqrt(K.mean(K.square(img_X_more - y_true), axis=-1)) \n",
    "        #loss2=K.sqrt(K.mean(K.square(lstm_X_more - y_true), axis=-1)) \n",
    "        #loss3=K.sqrt(K.mean(K.square(fused- y_true), axis=-1)) \n",
    "        loss1=mse(y_true,img_X_more)\n",
    "        loss2=mse(y_true,lstm_X_more)\n",
    "        loss3=mse(y_true,fused)\n",
    "        #0.2*loss1+0.2*loss2+\n",
    "        return 0.2*loss1+0.2*loss2+loss3\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(model.summary())\n",
    "    adam = optimizers.Adam(lr=1e-3, decay=5e-4)\n",
    "    model.compile( loss=root_mean_squared_error, optimizer=adam)\n",
    "    #return img_X,lstm_X,fused\n",
    "    if(multi_gpu == True):\n",
    "        parallel_model = multi_gpu_model(model, gpus=num_gpus)\n",
    "\n",
    "\n",
    "    \n",
    "        parallel_model.compile(optimizer= adam,\n",
    "                      loss=root_mean_squared_error, \n",
    "                      metrics=root_mean_squared_error)\n",
    "        return model, parallel_model\n",
    "    \n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def lstm_gen(x_train_feat, target,train_batch_size):\n",
    "    while True:\n",
    "        for batch in range(x_train_feat.shape[0] // train_batch_size + 1):\n",
    "            if batch > max(range(x_train_feat.shape[0] // train_batch_size)):\n",
    "                yield x_train_feat[batch*train_batch_size:],target[batch*train_batch_size:]\n",
    "            else:\n",
    "                yield x_train_feat[batch*train_batch_size:(1+batch)*train_batch_size],target[batch*train_batch_size:(1+batch)*train_batch_size]\n",
    "\n",
    "\n",
    "def img_gen(image_paths,  batch_size):  \n",
    "    \"\"\"\n",
    "    \n",
    "    Data generator for training data\n",
    "    \n",
    "    \"\"\"      \n",
    "         \n",
    "    while True:\n",
    "       \n",
    "        for batch in range(len(image_paths) // batch_size + 1):\n",
    "            X = []\n",
    "           \n",
    "            # choose random index in features\n",
    "            #index = np.random.choice(len(image_paths),1)[0]\n",
    "            if batch > max(range(len(image_paths) // batch_size)):\n",
    "                for index in range (batch*batch_size,len(image_paths)):\n",
    "            ##load image \n",
    "                    image = cv2.imread(image_paths[index])\n",
    "                    image = cv2.resize(image,(112,112))\n",
    "                    image=image*(1./225)\n",
    "                    X.append(image)\n",
    "            \n",
    "                    #y.append(labels[image_paths[index]]) #add the label for the image   \n",
    "    \n",
    "                yield np.array(X)\n",
    "            else:\n",
    "                for index in range (batch*batch_size,(batch+1)*batch_size):\n",
    "                    image = cv2.imread(image_paths[index])\n",
    "                    image = cv2.resize(image,(112,112))\n",
    "                    image=image*(1./225)\n",
    "                    X.append(image)\n",
    "                yield np.array(X)\n",
    "\n",
    "def merge_generator(img_generator, lstm_generator):\n",
    "        while True:\n",
    "            X1 = img_generator.__next__()\n",
    "            X2 = lstm_generator.__next__()\n",
    "            yield [X1, X2[0]], X2[1]\n",
    "            \n",
    "\n",
    "            \n",
    "df=pd.read_csv('tw_spydata_raw.csv')\n",
    "#print the head\n",
    "#df.head()\n",
    "\n",
    "img_dir=os.getcwd()+'/candle_img'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lstm_data=df[['Trade Close', 'Trade Volume']].copy()\n",
    "print(\"lstm_data: \",lstm_data.shape)\n",
    "\n",
    "feature_lst=[]\n",
    "target_lst=[]\n",
    "img_dir_lst=[]\n",
    "lstm_data=lstm_data.values.astype(float)\n",
    "\n",
    "lstm_data_reducer=lstm_data[1:]\n",
    "lstm_data_reduce=lstm_data[:-1]#latter, use lstm_data_reducer/lstm_data_reduce\n",
    "log_return_fea=np.log(lstm_data_reducer)-np.log(lstm_data_reduce)\n",
    "\n",
    "#scale with minmax\n",
    "#scaler=MinMaxScaler((-1,1))\n",
    "#scaler.fit(log_return_fea)\n",
    "#log_return_fea=scaler.transform(log_return_fea)\n",
    "\n",
    "\n",
    "lstm_data_reducer=lstm_data[34:,0]\n",
    "lstm_data_reduce=lstm_data[29:-5,0]\n",
    "log_return_tar=np.log(lstm_data_reducer)-np.log(lstm_data_reduce)\n",
    "print(\"log_return_fea.shape[0]\",log_return_fea.shape)\n",
    "print(\"log_return_tar:\",log_return_tar.shape)\n",
    "\n",
    "#for i in range(0,len(data)):\n",
    "#    new_data['Date'][i] = data['Date'][i]\n",
    "#    new_data['Close'][i] = data['Close'][i]\n",
    "\n",
    "#log_return_tar=log_return_fea[29:]\n",
    "for i in range(29,log_return_fea.shape[0]-4):\n",
    "    feature_lst.append(log_return_fea[i-29:i])\n",
    "    target_lst.append(log_return_tar[i-29])\n",
    "    img_dir_lst.append(img_dir+'/candle'+str(i-29)+'.png')\n",
    "   \n",
    "\n",
    "feature_lst=np.asarray(feature_lst)\n",
    "target_lst=np.asarray(target_lst)\n",
    "\n",
    "\n",
    "print(\"check nan:\",np.isnan(target_lst).any())\n",
    "print(\"check inf:\",np.isinf(target_lst).any() )\n",
    "#print(np.argwhere(np.isnan(target_lst)))\n",
    "#img_dir_lst=np.asarray(img_dir_lst)\n",
    "print(\"feature_lst shape (should same with target_lst): \",feature_lst.shape)\n",
    "#np.save('lstm_input',feature_lst)\n",
    "#np.save('target',target_lst)\n",
    "#np.save('lstm_input',feature_lst)\n",
    "\n",
    "train_size=int(feature_lst.shape[0]*0.7)\n",
    "val_size=int(feature_lst.shape[0]*0.1)\n",
    "test_size=feature_lst.shape[0]-train_size-val_size\n",
    "\n",
    "\n",
    "print(\"---start to create pyh5 file---\")\n",
    "h5f = h5py.File('data.h5', 'w')\n",
    "h5f.create_dataset('lstm_input_train', data=feature_lst[0:train_size])\n",
    "h5f.create_dataset('lstm_input_val', data=feature_lst[train_size:train_size+val_size])\n",
    "h5f.create_dataset('lstm_input_test', data=feature_lst[train_size+val_size:])\n",
    "h5f.create_dataset('target_train', data=target_lst[0:train_size])\n",
    "h5f.create_dataset('target_val', data=target_lst[train_size:train_size+val_size])\n",
    "h5f.create_dataset('target_test', data=target_lst[train_size+val_size:])\n",
    "\n",
    "h5f.close()\n",
    "print(\"---done---\")\n",
    "\n",
    "\n",
    "model=combine_model(False,0)\n",
    "\n",
    "final_train_gen = merge_generator(img_gen(img_dir_lst[0:train_size],batch_size_is), lstm_gen(feature_lst[0:train_size],target_lst[0:train_size], batch_size_is))\n",
    "final_val_gen = merge_generator(img_gen(img_dir_lst[train_size:train_size+val_size],batch_size_is), lstm_gen(feature_lst[train_size:train_size+val_size],target_lst[train_size:train_size+val_size], batch_size_is))\n",
    "\n",
    "\n",
    "print('debug1')\n",
    "filepath=\"weights_best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_sme', verbose=1, save_best_only=True, mode='max')\n",
    "history=model.fit_generator(final_train_gen,  # Features, labels\n",
    "                            steps_per_epoch=train_size//batch_size_is,\n",
    "                            epochs=epochs,\n",
    "                            validation_data=final_val_gen,\n",
    "                            validation_steps=val_size // batch_size_is\n",
    "                            callbacks=[checkpoint]\n",
    "                            #verbose=1 \n",
    "                            #callbacks=callbacks\n",
    "                            )  \n",
    "\n",
    "\n",
    "print('debug2')\n",
    "\n",
    "\n",
    "# save training history\n",
    "with open('my_model', 'wb') as file_1:\n",
    "    pickle.dump(history.history, file_1)\n",
    "#save model\n",
    "model.save_weights( './my_model_weights.h5')\n",
    "print('model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d6c63a31d3f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'my_model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile_1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('my_model', 'wb') as file_1:\n",
    "    pickle.dump(history.history, file_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6.5_tensorflow_keras_fastai_environment",
   "language": "python",
   "name": "python3.6.5_tensorflow_keras_fastai_environment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
